# 图表示学习

在本章中，我们研究几种在嵌入空间中表示图的方法。（在图上**表示学习**和**嵌入**指的是同一个事情，从不同角度的称呼。在下面的学习中，我们也会交替使用表示学习和嵌入这两个词语。）“嵌入”是指将网络中的每个节点映射到低维空间，这将使我们深入了解节点的相似性和网络结构。 鉴于图在网络和物理世界中的广泛流行，图的表示学习在广泛的应用中发挥着重要作用，例如链接预测和异常检测。 然而，现代机器学习算法是为简单的序列或网格（例如，固定大小的图像/网格或文本/序列）而设计的，网络通常具有复杂的拓扑结构和多模型特征。 我们将探索嵌入方法来克服困难。

## 节点表示学习

>   **解释:** 学习每个节点的低维表示

节点嵌入的目标是对节点进行编码，使得嵌入空间中的相似性（例如点积）近似于原始网络中的相似性，我们将探索的节点嵌入算法通常由三个基本阶段组成：

1.  定义一个编码器（即从节点到嵌入的映射）。 下面我们用一张图来说明这个过程，编码器 $ENC$ 将节点 $u$ 和 $v$ 映射到低维向量 $z_u$ 和 $z_v$。
2.  定义节点相似度函数（即原始网络中相似性的度量），它指定向量空间中的关系如何映射到原始网络中的关系。
3.  优化编码器的参数，使得 $u$ 和 $v$ 在嵌入空间的相似度 $similarity(u, v)=z_u^{T}z_v$ 更高。

<img src="https://datawhalechina.github.io/grape-book/figures/04%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/4_node_embeddings.png" alt="img" style="zoom: 25%;" />

### 一般的随机游走: 深度游走(DeepWalk)

>   *深度游走的思想可以概括为：*先根据相似度计算节点的转移概率，目标是使得"相邻"的节点转移概率尽可能大

这里我们介绍随机游走：

-   给定一个图和一个起点，我们随机选择它的一个邻居，并移动到这个邻居；
-   然后我们随机选择该点的邻居，并移动到它，以此类推。

以这种方式随机选择的点的序列就是图上的随机游走。

在随机游走中，相似度 $similarity(u, v)=z_u^{T}z_v$ 被定义为 $u$ 和 $v$ 在一个随机游走时同时出现的概率。

<img src="https://datawhalechina.github.io/grape-book/figures/04%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/4_random_walk.png" alt="img" style="zoom: 67%;" />

随机游走是一个比较宽泛的概念，只要它满足随机游走的想法。但是**深度游走**算法特指运行固定长度、无偏的随机游走。我们可以按照以下步骤进行深度游走：

1.  从节点 $u$ 开始采用随机游走策略 $R$ 进行随机游走，得到附近的节点为 $N_R(u)$。最简单的想法是从每个节点开始运行固定长度、无偏的随机游走，这就是深度游走。
2.  由于我们希望在嵌入空间中使得附近的节点嵌入相似度高，因此我们需要进行嵌入的优化，以使附近的节点在网络中靠近在一起。 我们可以优化嵌入以最大化随机游走共现的可能性，其损失函数为：

$$
\mathcal{L} = \sum_{u \in V}\sum_{v \in N(u)}-\log \mathbb{P}\{v \vert z_u\}
$$

​	其中的后验概率 $\mathbb{P}\{v \vert z_u\}$ 可以被表示为 $\mathbb{P}\{v \vert z_u\} = \frac{\exp (z_u^{T} z_v)}{\sum_{n\in V}\exp(z_u^{T}z_n)}$。因为我们可以得到优化目标为:
$$
\mathcal{L} = \sum_{u \in V}\sum_{v \in N(u)}-\log \frac{\exp (z_u^{T} z_v)}{\sum_{n\in V}\exp(z_u^{T}z_n)}
$$

>   **解释:**
>
>   假设从网络中任意地选择两个节点 $u, v$, 设它的低维表示(嵌入)为 $z_u, z_v$, 那么在随机游走的过程中，节点 $u$ 跳转到节点 $v$ 的概率可以根据它们的相似度 $similarity(u, v) = z_u^{T}z_v$ 来计算:
>   $$
>   \mathbb{P}\{v \vert z_u\} = \frac{\exp (z_u^{T} z_v)}{\sum_{n\in V}\exp(z_u^{T}z_n)}
>   $$
>   而根据原始图中节点的邻接关系，我们希望在嵌入空间中使得附近的节点嵌入相似度高，即原始图中相邻的节点在低维空间中尽可能地相近 -> 在一次随机游走中共现的可能性最大(最大化似然函数), 即:
>   $$
>   \max  \prod_{u \in V}\prod_{v \in N(u)}\mathbb{P}\{v \vert z_u\}
>   $$
>   *"对于图中所有的节点，我们希望它和它邻居节点共现的概率最大"*
>
>   采用概率论中对似然函数常用的处理手段，即最大化似然与最小化负对数似然等价，从而可以定义损4失函数如下:
>   $$
>   \mathcal{L} = \sum_{u \in V}\sum_{v \in N(u)}-\log \mathbb{P}\{v \vert z_u\}
>   $$
>   联立式 $(1)$ 和式 $(3)$, 可以得到最终的优化目标如下:
>   $$
>   \mathcal{L} = \sum_{u \in V}\sum_{v \in N(u)}-\log \frac{\exp (z_u^{T} z_v)}{\sum_{n\in V}\exp(z_u^{T}z_n)}
>   $$

但是，想要优化上面这个目标太昂贵（复杂度为 $O(\vert V \vert^{2})$），因为我们需要计算其分母中每两个节点的相似度。这里我们引入一下**负采样**。具体地，我们引入一个表示所有点的随机概率 $\mathbb{P}_v$。这样我们就不需要计算所有节点 $u$ 和所有点的相似度，而是只计算 $k$ 个随机采样得到的负样本 $n_i$。
$$
\log (\frac{\exp (z_u^{T}z_v)}{{\sum_{n\in V}\exp(z_u^{T}z_n)}}) \approx \log(\sigma(z_u^{T}z_v)) - \sum_{i=1}^{k}\log\Big(\sigma(z_u^{T}z_{n_i})\Big), \quad n_i \sim \mathbb{P}_v
$$
上面的损失函数是噪声对比估计 (Noice Contrastive Estimation, NCE) 的一种形式，可使用逻辑回归（sigmoid 函数）近似最大化 softmax 的对数概率。 请注意，较高 $k$ 能给出了更鲁棒的估计。在实际应用中，我们选择 $k$ 的值在 $5$ 到 $20$ 之间。

>   **解释:**
>
>   这里的意思是说，每轮随机游走都要计算整个图上任意两个节点的相似度(内积)，对于一个大规模图(例如有数百万个节点的社交网络)来说开销太大。我们的优化目标仅仅是让"相邻"的节点在低维空间中尽可能地接近(即共现概率最大)。但这里的接近是一个相对的概念，实际上我们可以让相邻节点共现的概率显著高于不相邻的节点，也就是说只关心概率的比较关系，而不关心具体的概率值。所以可以通过负采样的方式，在网络中随机采样 $k$ 个节点，用于近似估计式 $(2)$ 中的分母，从而对概率的关系进行近似估计。
>
>   **注意:** 网络中任意一个节点被采样的概率是服从 $\mathbb{P}_v$ 的

### 有偏的随机游走：Node2Vec

最简单的随机游走策略是**深度游走**，即从每个节点开始运行固定长度、无偏的随机游走。但是，这种游走策略太死板，会限制表征的学习。Node2Vec 提出了一种更高效的、灵活的、有偏的随机游走策略，以得到一个更好的 $N_R(u)$。Node2Vec 通过图上的广度优先遍历（Breath First Search, BFS）和深度优先遍历（Depth First Search, DFS）在网络的局部视图和全局视图之间进行权衡。

下面，我们简单复习一下 DFS 和 BFS 的基本概念。BFS 可以给出邻域的局部微观视图，而 DFS 提供邻域的全局宏观视图。 这里我们可以定义返回参数 $p$ 来代表模型返回前一个节点的转移概率和输入输出参数 $q$ 定义 BFS 和 DFS 的“比率”。当 $p$ 的值比较小的时候，Node2Vec 像 BFS；当 $q$ 的值比较小的时候，Node2Vec 像 DFS。

<img src="https://datawhalechina.github.io/grape-book/figures/04%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/4_node2vec.png" alt="img" style="zoom:80%;" />

我们使用二阶随机游走来获得邻居节点集 $N_R(u)$。以下图所示，如果我们刚刚从节点 $S_1$ 到达节点 $W$，那么我们访问在下一个时刻访问各个节点的权重为：
$$
\alpha_{pq} = 
\left \{
\begin{array}{cl}
\frac{1}{p} & \mathrm{if} \ d_{s_1,x} = 0 \\
1 & \mathrm{if} \ d_{s_1, x}=1\\
\frac{1}{q} & \mathrm{if} \ d_{s_1, x} = 2
\end{array}
\right .
$$
即访问 $S_2$ 的权重为 $1$，访问 $S_3$ 和 $S_4$ 的权重为 $\frac{1}{q}$，返回 $S_1$ 的权重为 $\frac{1}{p}$，对权重进行归一化后即可得到对应的概率
<img src="https://datawhalechina.github.io/grape-book/figures/04%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/4_biased_walk.png" alt="img"  />

>   **解释**:
>
>   参数 $p$ 控制了随机游走中节点的返回概率，$p$ 值越大，则一次随机游走中再次访问已经访问过的节点的概率越低，这保证了随机游走的"探索性"；相应地，如果 $p$ 值越小，则再次访问已经访问的节点的概率越高，从而随机游走呈现"局部化"
>
>   参数 $q$ 控制了DFS和BFS的倾向性，具体来说，$q$ 值越大，则节点更倾向于游走到距离自身更近的节点，即与BFS类似，而 $q$ 值越小，则节点更倾向于游走到距离自身更远的节点，即与DFS类似

现在，让我们来总结一下 Node2Vec 算法：

1.  计算随机游走概率
2.  针对每个节点 $u$ 模拟 $r$ 个从节点 $u$ 开始长度为 $l$ 的随机游走
3.  使用随机梯度下降优化 Node2Vec 目标

<img src="C:\Users\weepcat\AppData\Roaming\Typora\typora-user-images\image-20240417104130660.png" alt="image-20240417104130660" style="zoom: 50%;" />

## 图表示学习

我们可能还想在某些应用中嵌入整个图 $G$（例如，对有毒分子与无毒分子进行分类、识别异常图）。

>   **解释:** 学习每个图的低维表示

<img src="https://datawhalechina.github.io/grape-book/figures/04%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/4_graph_embedding.png" alt="img" style="zoom:80%;" />

完成图嵌入有几种想法：

1.  简单的想法是在图 $G$ 上运行图的节点嵌入技术，然后对图 $G$ 中的节点嵌入求和（或平均）。
2.  引入“虚拟节点”来表示图并运行标准图节点嵌入技术：
3.  我们还可以使用匿名步行嵌入。 为了学习图嵌入，我们可以枚举所有可能的匿名游走，并记录它们的计数，然后将图表示为这些游走的概率分布。

## References

[1] Jure Leskovec, [Stanford University CS224W: Machine Learning with Graphs](https://web.stanford.edu/class/cs224w/)

[2] 包勇军、朱小坤、颜伟鹏、姚普，[图深度学习从理论到实践](http://www.tup.tsinghua.edu.cn/Wap/tsxqy.aspx?id=09165201)，清华大学出版社，2022

[3] Datawhale，[图深度学习 （葡萄书）第四章 图表示学习](https://datawhalechina.github.io/grape-book/#/)

[4] Grover, A., & Leskovec, J. (2016). *node2vec: Scalable Feature Learning for Networks* (arXiv:1607.00653). arXiv. http://arxiv.org/abs/1607.00653

