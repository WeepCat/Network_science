# 图理论基础

## 图的定义

-   一个**图**被记为 $G=\{V, E\}$。
-   其中 $V=\{v_1, \ldots, v_N\}$ 是数量为 $N=\vert V \vert$ 的**节点（node或vertex）的集合**，$E=\{e_1, \ldots, e_{M} \}$ 是数量为$M$ 的**边（edge或link）的集合**。
-   图用节点表示实体（entities），用边表示实体间的关系（relations）。
-   假如一条边 $e \in E$ 连接两个节点 $v_1$和 $v_2$，那么这条边可以被表示为 $e=(v_1, v_2)$。
-   节点和边的信息可以是**类别型**的（categorical），类别型数据的取值只能是哪一类别。一般称类别型的信息为**标签（label）**。
-   节点和边的信息可以是**数值型**的（numeric），数值型数据的取值范围为实数。一般称数值型的信息为**属性（attribute）**。
-   在图的计算任务中，我们认为，节点一定含有信息（至少含有节点的度的信息），边可能含有信息。

图根据它的边是否具有指向性可以分为：

-   **有向图（directed graph or digraph）**：有向图的边是具备指向性的。
-   **无向图（undirected graph）**：无向图的边不具备指向性。

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_digraph.PNG" alt="img" style="zoom:67%;" />

根据图的边上权重是否为 $1$，我们可以将它们分为：

-   图的边上的权重为 $1$ 时，它是一个无权图（unweighted graph）。
-   图的边上的权重不为 $1$ 的时候，它是一个有权图（weighted graph）。我们记点  $v_i$ 到 $v_j$ 的权重为 $w_{ij}$

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_weighted_graph.PNG" alt="img" style="zoom:67%;" />

## 图的性质

### 邻接节点（neighbors）

-   **节点 $v_i$ 的邻接节点**是与节点 $v_i$ 直接相连的节点，其被记为 $N(v_i)$。
    -   节点 $v_i$ 的 $k$ 跳远的邻接节点（neighbors with $k$-hop）是到节点 $v_i$ 要走 $k$ 步的节点（一个节点的 $2$ 跳远的邻接节点包含了自身）。

### 图的度 （degree）

-   节点 $v_i$ 的度记为 $d(v_i)$，入度记为 $d_{in}(v_i)$，出度记为 $d_{out}(v_i)$。

-   对于**有向有权图**：节点 $v_i$ 的出度（out degree）等于从 $v_i$ 出发的边的权重之和；节点 $v_i$ 的入度（in degree）等于从连向 $v_i$ 的边的权重之和。

-   **无向图**是有向图的特殊情况，节点的出度与入度相等。

-   **无权图**是有权图的特殊情况，各边的权重为 $1$，那么节点 $v_i$ 的出度（out degree）等于从 $v_i$ 出发的边的数量，节点 $v_i$ 的入度（in degree）等于从连向 $v_i$ 的边的数量。

-   **平均度**是一个表达网络整体性质重要的参数。对于无向图来说，平均度的计算为:
    $$
    \bar{d}(G) = \frac{1}{N}\sum_{i=1}^{N}d_i = \frac{2M}{N}
    $$

-   **度分布 $P(d)$** 表示随机选择的节点的度为 $d$ 的概率，平均度

$$
\bar{d}(G) = \sum_{i=1}^{\infty}dP(d)
$$

### 行走（walk）和路径（path）

-   $walk(v_1,v_2)=(v_1,e_6,e_5,e_4,e_1,v_2)$，这是一次“行走”，它是一次从节点 $v_1$ 出发，依次经过边 $e_6,e_5,e_4,e_1$ 终到达节点 $e_2$ 的“行走”。
-   下图所示为 $walk(v_1,v_2)=(v_1,e_6,e_5,e_4,e_1,v_2)$，其中红色数字标识了边的访问序号。
-   在“行走”中，节点是允许重复的。
-   **路径**是节点不可重复的**行走**。

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_walk.png" alt="img" style="zoom: 33%;" />

### 距离（distance）、直径（diameter）

-   最短路径被定义为两个点之间的**距离（distance）**。**最短路径（shortest path）** $v_s,v_t \in V$ 是图 $G=\{V,E\}$ 上的一对节点，节点对 之间 $v_s,v_t \in V$ 所有路径的集合记为 $p_{st}$。节点对 $v_s,v_t$ 之间的最短路径 $p_{st}^{sp}$ 为 $p_{st}$ 中长度最短的一条路径，其形式化定义为

$$
p_{st}^{sp} = \arg\max_{p\in p_{st}} \vert p \vert
$$

​	其中，$p$ 表示 $p_{st}$ 中的一条路径，$\vert p \vert$ 是路径 $p$ 的长度。

-   **直径（diameter）**：给定一个连通图 $G=\{V, E\}$，其直径为其所有节点对之间的**最短路径的最大值**，形式化定义为

$$
diameter(G) = \max_{v_s, v_t 
\in V} \min_{p \in p_{st}} \vert p \vert
$$

### 子图（subgraph）、连通分量（connected component）、连通图（connected graph）

-   **子图（subgraph）**：有一图 $G=\{V, E\}$ ，另有一图 $G'=\{V', E'\}$，其中 $V' \in V, E' \in E$ 并且 $V'$ 不包含 $E'$ 中未出现过的节点，那么 $G'$ 为 $G$ 的子图。

-   **连通分量（connected component）**：给定图 $G'=\{V', E'\}$ 是图  $G=\{V, E\}$的子图。记属于图 $G$ 但不属于 $G'$ 图的节点集合记为 $V/V'$ 。如果属于 $V'$ 的任意节点对之间存在至少一条路径，但不存在一条边连接属于 $V'$ 的节点与属于 $V/V'$ 的节点，那么图 $G'$是图 $G$ 的连通分量。
-   **连通图（connected graph）**：当一个图只包含一个连通分量，即其自身，那么该图是一个连通图。

### 聚类系数（Clustering Coefficient）

-   聚类系数表示给定节点的**邻居彼此链接的程度**。
-   节点 $i$ 的邻域互连越紧密，其局部聚类系数越高。
-   $C_i$ 是节点的两个邻居相互链接的概率。
-   对于度数为 $d_i$ 的节点 $i$，**局部聚类系数**定义为

$$
C_i = \frac{E_i}{T_i}
$$

其中，$E_i$ 表示节点 $i$ 的邻居实际存在的边的数量，$T_i$ 表示节点 $i$ 的邻居可能（最多）存在的边的数量。

-   $C_i=0$ 如果节点 $i$ 的邻居都没有相互链接。
-   $C_i = 1$ 如果节点 $i$ 的邻居形成一个全连接图，即它们都相互链接。
-   $C_i = 0.5$ 意味着一个节点的两个邻居有 $50\%$ 的机会链接。
-   **网络的聚类系数**即**平均聚类系数**：是所有节点的集聚系数的平均值为

$$
C = \frac{1}{N}\sum_{i}C_i
$$

### 接近中心度 (closeness centrality)

-   在连通图中，节点的**接近中心性**（或接近性）是网络中中心性的度量，计算为该节点与图中所有其他节点之间的最短路径长度之和的倒数。
-   节点越中心，它与所有其他节点越接近。
-   接近中心度的计算公式为

$$
c(v) = \frac{1}{\sum_{u \ne v} \mathrm{shortest \ path \ length\ between\ u\ and\ v}}
$$

## 图的连接表示

下面我们会介绍邻接矩阵、关联矩阵和拉普拉斯矩阵。一些文献会把他们归入图的性质中，但是因为它们是后续图神经网络中使用的重点，我们单独对它们进行更详细的讲解。

### 邻接矩阵 （Adjacency Matrix）

-   给定一个图 $G=\{V, E\}$，其对应的**邻接矩阵**被记为 $\textbf{A} \in \{0, 1\}^{N \times N}$。
-   $\textbf{A}_{ij}=1$ 表示存在从节点 $v_i$ 到 $v_j$ 的边，$\textbf{A}_{ij}=0$ 表示不存在从节点 $v_i$ 到 $v_j$ 的边。
-   在**无向图**中，从节点 $v_i$ 到 $v_j$ 的边存在，意味着从节点 $v_j$ 到 $v_i$ 的边也存在。因而**无向图的邻接矩阵是对称的**。
-   在**无权图**中，**各条边的权重被认为是等价的**，即认为**各条边的权重为 $1$**。
-   对于**有权图**，其对应的邻接矩阵通常被记为 $\textbf{W} \in \R^{N \times N }$，其中 $\mathrm{W}_{ij} = w_{ij}$ 表示从节点 $v_i$ 到 $v_j$ 的边的权重。若边不存在时，边的权重为 $0$ 。

一个无向无权图的例子（左边为图，右边为图的邻接矩阵）：

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_adjacency_matrix.png" alt="img" style="zoom:67%;" />

### 关联矩阵（Incidence Matrix）

-   给定一个图 $G=\{V, E\}$，其对应的**关联矩阵**被记为 $\textbf{M} \in \{0, 1\}^{N \times M}$。（这里我们用加粗的 $\textbf{M}$ 表示关联矩阵，用不加粗的 $M$ 表示边的个数）
-   $\textbf{M}_{ij}=1$ 表示节点 $v_i$ 和边 $e_j$ 相连接, $\textbf{M}_{ij}=0$ 表示节点 $v_i$ 和边 $e_j$ 不相连接。
-   与邻接矩阵不同，关联矩阵描述的是定点和边之间的关系。

一个无向无权图的例子（左边为图，右边为图的关联矩阵）：

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_incidence_matrix.PNG" alt="img" style="zoom:67%;" />

### 拉普拉斯矩阵（Laplacian Matrix）

-   **拉普拉斯矩阵（Laplacian Matrix）**：（也叫做 admittance matrix, Kirchhoff matrix）给定一个图 $G=\{V, E\}$，其邻接矩阵为 $\textbf{A}$，其拉普拉斯矩阵 $\textbf{L}$ 定义为 $\textbf{L} = \textbf{D} - \textbf{A}$ 其中 $\textbf{D} = \textbf{diag}(\textbf{d}(v_1), \ldots, \textbf{d}(v_N))$ 是度矩阵。更具体地，我们记拉普拉斯矩阵中每一个元素为 $\textbf{L}_{ij}$，那么每一个元素可以被定义为
    $$
    \textbf{L}_{ij} = \left \{
    
    \begin{array}{lll}
    d_i, & \mathrm{if} \ i=j \\
    -1, & \mathrm{if} \ i \ne j \ \mathrm{and} \ v_i \ \mathrm{adjacent \ with} \ v_j \\
    0, & \mathrm{otherwise}
    \end{array} 
    
    \right .
    $$

它的每一行和列的加和为 $0$。

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_laplacian_matrix.PNG" alt="img" style="zoom:67%;" />

-   **对称归一化的拉普拉斯矩阵，Symmetric normalized Laplacian）**：给定一个图 $G=\{V, E\}$，其邻接矩阵为，$\textbf{A}$, 其规范化（归一化）的拉普拉斯矩阵定义为:
    $$
    \textbf{L} = \textbf{D}^{-\frac{1}{2}}(\textbf{D} - \textbf{A})\textbf{D}^{-\frac{1}{2}} = \text{I} - \textbf{D}^{-\frac{1}{2}}\textbf{A}\textbf{D}^{-\frac{1}{2}}
    $$

    ## 图的类型

    按照不同的划分规则，图可以被划分为很多不同的种类。我们可以根据边是否具有指向性，将图分为有向图和无向图。下面，我们会根据更多不同的属性来对图进行划分。

    ### 图的拓扑结构

    根据图的拓扑结构，规则网络（regular network）可以分为

    -   全连接网络（fully-connected network）（下图左）
    -   环形网络（ring-shape network）（下图中）
    -   星形网络（start-shape network）（下图右）

    <img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_network_topology_regular.PNG" alt="img" style="zoom:67%;" />

根据一些其他的不同性质，常见的图模型还有随机图（random graph）、小世界图（small world graph）和无标度图模型（scale-free graph）。

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_network_topology_other.png" alt="img" style="zoom: 80%;" />

### 同质图和异质图

深度学习中的同质图和异质图和原本图理论中的定义稍有不同，这里我们只给出在深度学习中更常见的定义。

-   **同质图**（Homogeneous Graph）：只有一种类型的节点和一种类型的边的图。
-   **异质图**（Heterogeneous Graph）：存在多种类型的节点和多种类型的边的图。

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_heterogeneous.png" alt="img" style="zoom: 33%;" />

### 二分图 （bipartite graph）

二分图或二部图（Bipartite Graphs）：节点分为两类，只有不同类的节点之间存在边。

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_bipartite_real.png" alt="img width=&quot;50&quot; height=&quot;50&quot;" style="zoom: 25%;" />

更具体地，二分图是一个网络，其节点可以分为两个不相交的集合 $U, V$，使得每个链接将 $U$ 节点连接到 $V$ 节点。换句话说，如果我们将 $U$ 节点着色为绿色，将 $V$ 节点着色为紫色，那么每个链接必须连接不同颜色的节点。我们可以为每个二分网络生成两个投影。如果两个 $U$ 节点链接到二分表示中的相同 $V$ 节点，则第一个投影通过链接连接两个 $U$ 节点。如果它们连接到相同的 $U$ 节点，则第二个投影通过链接连接 $v$ 节点

<img src="https://datawhalechina.github.io/grape-book/figures/02%E5%9B%BE%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/2_bipartite_network_science.PNG" alt="img" style="zoom: 50%;" />

## References

[1] 包勇军、朱小坤、颜伟鹏、姚普，[图深度学习从理论到实践](http://www.tup.tsinghua.edu.cn/Wap/tsxqy.aspx?id=09165201)，清华大学出版社，2022

[2] Albert-László Barabási, [Network Science](http://networksciencebook.com/chapter/2)

[3] Guanrong Chen, [Fundamentals of Complex Networks: Models, Structures and Dynamics](https://www.amazon.com/Fundamentals-Complex-Networks-Structures-Dynamics/dp/1118718119)

[4] Jure Leskovec, [Stanford University CS224W: Machine Learning with Graphs](https://web.stanford.edu/class/cs224w/)

[5] Yao Ma and Jiliang Tang, [Deep Learning on Graphs](https://web.njit.edu/~ym329/dlg_book/), 2021

[6] 马耀、汤继良，[图深度学习（Deep Learning on Graphs 中文版）](https://item.jd.com/13221338.html)

[7] Datawhale，[图神经网络组队学习](https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN/)

[8] Datawhale，[图深度学习 （葡萄树）第二章 图理论基础](https://datawhalechina.github.io/grape-book/#/)

