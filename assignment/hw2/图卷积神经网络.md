# 图卷积网络

>   Network Embedding
>
>   先前我们已经了解了如何通过随机游走(Deepwalk, Node2Vec等)学习节点和图的表示(类似地，也可以得到边的表示)，即节点在低维空间中进行随机游动时，要求原始图上相邻的节点共现概率尽可能大 
>
>   但是这些方法存在几个比较大缺点。第一，参数数量与节点数量成正比，假如图中节点个数为 $N$ ，表征向量大小为 $M$，总参数量为$N \times M$，因此随着图节点个数不断增加参数量也会随之增加(GNN可以通过参数共享方式计算表征向量)。第二，无法利用节点的特征信息，只是使用了节点索引index信息。第三，对于没有出现在训练数据中的节点无法进行表征(GNN可以通过Inductive representation Learning解决这个问题)。
>
>   CNN的出现提供了一个新的学习范式，如何在图上定义一个卷积操作，从而套用CNN的范式来处理图数据？
>
>   本章重点关注如何在图上定义一个卷积操作，从而利用图卷积神经网络学习节点和图的表示，进一步地应用在下游任务上

现有图神经网络皆基于邻居聚合的框架，即为每个目标节点通过聚合其邻居刻画结构信息，进而学习目标节点的表示。因此，在建模图神经网络时，研究人员的关注重点是如何在网络上构建聚合算子，聚合算子的目的是刻画节点的局部结构。现有的聚合算子构建分为两类：

-   谱域方法：利用图上卷积定理从谱域定义图卷积。
-   空间域方法：从节点域出发，通过在节点层面定义聚合函数来聚合每个中心节点和其邻近节点。主要包括空域图卷积神经网络、GraphSAGE和图注意力网络。

## 谱域图卷积神经网络

谱域图卷积神经网络主要包括谱卷积神经网络、切比雪夫网络和图卷积神经网络。我们会先从信号与系统的角度讲解谱图理论，从而得到最早的谱卷积神经网络，然后通过简化得到切比雪夫网络，最后进行再次简化得到图卷积神经网络。

### 谱图理论和图卷积

过去几年，卷积神经网络在图像领域大放异彩。卷积算子定义了加权和操作，其本身是聚合算子。借助于卷积神经网络对局部结构的建模能力以及网络数据上普遍存在的节点依赖关系，通过定义网络数据上的卷积算子进而设计图神经网络已经成为其中最活跃最重要的一支。但网络数据上平移不变性的缺失，给在节点域定义卷积算子带来困难。谱方法利用卷积定理从谱域定义卷积算子。 我们首先给出卷积定理的背景知识。

**卷积的傅里叶变换**

卷积定理：信号卷积的傅立叶变换等价于信号傅立叶变换的乘积
$$
\mathcal{F}(f \ast g)=\mathcal{F}(f) \cdot \mathcal{F}(g)
$$
其中的 $f, g$ 表示两个原始信号，$\mathcal{F}(f)$ 表示 $f$ 的傅立叶变换，$\cdot$ 表示乘积算子， $\ast$ 表示卷积算子。

对上面的公式做傅立叶逆变换，可以得到
$$
f \ast g = \mathcal{F}^{-1}\Big(\mathcal{F}(f) \cdot \mathcal{F}(g)\Big)
$$
其中 $\mathcal{F}^{-1}(f)$ 表示信号 $f$ 的傅立叶逆变换。

利用卷积定理，我们可以对谱空间的信号做乘法，再利用傅里叶逆变换将信号转换到原空间来实现图卷积，从而避免了图数据不满足平移不变性而造成的卷积定义困难问题。

**图傅里叶变换**

图傅立叶变换依赖于图上的拉普拉斯矩阵 $\mathbf{L}$。对 $\mathbf{L}$ 做谱分解，我们可以得到:
$$
\mathbf{L} = U\Lambda U^{T}
$$
其中 $\Lambda = \mathrm{diag}(\lambda_0, \ldots, \lambda_{N-1}) \in \R^{N \times N}$ 是特征值矩阵, $U = [u_0, \ldots, u_{N-1}] \in \R^{N \times N}$ 是对应的特征向量矩阵。如下图所示

![img](https://datawhalechina.github.io/grape-book/figures/05%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/5_spectral_decomposition.png)

图上傅立叶变换的定义依赖于拉普拉斯矩阵的特征向量。以特征向量作为谱空间下的一组基底，定义图上信号 $x$ 的傅立叶变换为：
$$
\hat{x} = U^{T}x
$$
其中 $x$ 指信号在节点域的原始表示。$\hat{x}$ 指信号 $x$ 变换到谱域后的表示，$U^{T}$ 表示特征向量矩阵的转置，用于做傅立叶变换。信号 $x$ 的傅立叶逆变换为:
$$
x = U\hat{x}
$$

>   **解释**
>
>   因为拉普拉斯矩阵是对称的，实对称阵一定能够对角化(特征分解)，所以一定存在 $N$ 个线性无关的特征向量构成矩阵 $U$。而这 $N$ 个线性无关的特征向量 $u_0, \ldots, u_{N-1}$ 构成了 $n$ 维线性空间 $V$ 的一组基。那么图上信号 $x$ 的傅里叶变换相当于把信号 $x$ 投影到线性空间 $V$ 中，而这个投影变换是可逆的
>
>   这个线性空间 $V$ 被称为**谱域**
>   $$
>   U\hat{x} = UU^{T}x = x, \quad (U是正交矩阵，UU^{T} = 1)
>   $$

**图卷积**

为了完成图上的卷积操作，我们要做的事情就是，**先将图进行傅里叶变化，在谱域完成卷积操作，然后再将频域信号转换回原域**。具体流程如下: 设 $x$ 是图上的一个信号，$y$ 是卷积核，则图上的卷积操作可以写成(类比式 $(2)$ ):
$$
x \ast y = U\Big((U^{T}x) \odot  U^{T}y\Big)
$$
进一步，可以用参数 $\theta$ 表示 $U^{T}y$，记为 $g_{\theta}$，由此就定义了一个图上的卷积操作:
$$
x \ast y = U g_{\theta} U^{T}x
$$
其中，$g_\theta$ 是可学习参数，并且是一个对角矩阵的形式(将向量与向量的hadamard product转为矩阵乘法)。以上过程可以被表示为下图:

<img src="https://datawhalechina.github.io/grape-book/figures/05%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/5_graph_conv.png" alt="img" style="zoom: 80%;" />

基于此卷积算子的定义，国内外陆续涌现出一些基于卷积聚合的图神经网络。

### 谱卷积神经网络

谱卷积神经网络（Spectral Convolutional Neural Network）是最早提出在网络数据上构建图神经网络的方法，该方法完全按照上述得到的卷积操作，堆叠多层得到。我们可以将上面公式的 $x$ 替换为第 $l$ 层网络中输入的节点特征 $H^{(l)}$，而经过卷积操作后的第 $l$ 层网络的输出为:
$$
H^{(l+1)} = \sigma(Ug_{\theta}U^{T}H^{(l)})
$$
其中，$\sigma$ 表示非线性激活函数。

>   **说明**
>
>   式 $(9)$ 定义的图卷积操作存在以下缺陷:
>
>   1.   卷积核的参数为 $n$，并不是局部化的。以CNN对图像进行卷积操作为例，相当于定义的卷积核是和完整的一张图片进行卷积，而不是图片的一个patch
>   2.   需要对拉普拉斯矩阵进行特征分解，增加了计算量

### 切比雪夫网络

谱卷积神经网络基于全图的傅里叶卷积来实现图的卷积，其缺点非常明显，难以从卷积形式中保证节点的信息更新由其邻居节点贡献，因此无法保证局部性。另外，谱卷积神经网络的计算复杂度比较大，难以扩展到大型图网络结构中。切比雪夫网络 (ChebyNet)，采用切比雪夫多项式替代了谱卷积神经网络的卷积核，有效的解决了上述的问题。

在谱卷积神经网络中，$g_{\theta}$ 为对角阵的形式，且有 $n$ 个可学习的参数。切比雪夫网络 (ChebyNet) 利用切比雪夫多项式对 $g_{\theta}$ 进行参数化:
$$
g_{\theta} = \sum_{i=0}^{K-1} \theta_kT_k(\hat{\Lambda})
$$
其中, $\theta_k$ 是需要学的系数，并定义 $\hat{\Lambda} = \frac{2\Lambda}{\lambda_{\max}}-I_n$。切比雪夫多项式是可以通过递归求解，递归表达式为:
$$
T_{k}(x) = 2xT_{k-1}(x) - T_{k-2}(x)
$$
其中初始值 $T_0(x)=1, T_1(x)=x$

令 $\hat{\mathbf{L}} = \frac{2\mathbf{L}}{\lambda_{\max}} - I_n$, 切比雪夫网络第 $l$ 层的结构定义如下：
$$
\begin{aligned}
H^{(l+1)} &= \sigma \Bigg(U\Big(\sum_{i=1}^{K-1}\theta_kT_k(\hat{\Lambda})\Big) U^{T}H^{(l)} \Bigg) \\
&= \sigma \Bigg(\Big(\sum_{i=1}^{K-1}\theta_kT_k(\hat{\mathbf{L}})\Big)H^{(l)} \Bigg)
\end{aligned}
$$
切比雪夫网络利用特征值矩阵的多项式参数化卷积核，实现谱卷积神经网络，且巧妙的利用 $\mathbf{L} = U\Lambda U^{T}$ 引入拉普拉斯矩阵，从而避免了拉普拉斯矩阵的特征分解，同时参数复杂度从 $O(n \times p \times q)$ 下降到 $O(K \times p \times q)$。此外，在拉普拉斯矩阵中，当且仅当两个节点满足 $K$ 跳可达时，其拉普拉斯矩阵中这一项不为 $0$，这一性质使得当 $K$ 较小时，切比雪夫网络具有局部性。

>   **说明**
>
>   1.   Chebynet库用特征值构成的对角矩阵，避免了对拉普拉斯矩阵进行特征分解，降低了计算复杂度
>
>   2.   卷积核的参数量与节点个数 $n$ 无关，是局部化的
>
>   **为什么可以这样定义?**
>
>    $g_{\theta}$ 原先可以从任意的一个函数中进行学习，而多项式函数可以通过提高阶数的方式逼近任意函数，所以考虑采用多项式函数的形式。而为了避免特征分解，将多项式的基底选择为对角阵 $\Lambda$ 

### 图卷积神经网络

图卷积神经网络（Graph Convolutional Network, GCN）对切比雪夫网络进行了简化，只取 $0$ 阶和 $1$ 阶，并且令拉普拉斯矩阵 $\mathbf{L}$ 是归一化的，即 $\mathbf{L} = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$, 此时 $\lambda_{\max}(\mathbf{L}) \approx 2$ , 故 $g_{\theta}$ 的形式如下:
$$
\begin{aligned}
g_{\theta} &= \sum_{i=0}^{1} \theta_kT_k(\hat{\mathbf{L}})\\
&= \theta_0 I_N + \theta_1 \hat{\mathbf{L}} \\
&= \theta_0I_N + \theta_1(\mathbf{L} - I_N) \\
&= \theta_0I_N - \theta_1\Big(D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \Big) \\
&= \theta_0 I_N + \theta_1\Big(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\Big) \quad (\theta_1是可学习的，故符号无影响)
\end{aligned}
$$

进一步对参数进行约简，可以令:
$$
g_{\theta} = \theta \Big(I_N +  D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \Big)
$$
再利用一个重参数化技巧避免可能存在的梯度爆炸/消失问题:
$$
I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}
$$
其中，$\tilde{A} = A + I_N, \tilde{D} = \sum_{j}\tilde{A}_{ij}$，从而信号 $x$ 与卷积核 $y$ 的卷积被近似为:
$$
x \ast y \approx \theta \cdot \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}x
$$
当 $\theta$ 取不同值时，分别代表了不同的卷积核。故图卷积神经网络第 $l$ 层的结构定义如下:
$$
\begin{aligned}
H^{(l+1)} &= \sigma \Big(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W \Big)
\end{aligned}
$$

其中，$H^{(l)} \in \R^{N\times C}$ 表示第 $l$ 层网络中输入的节点特征，$W \in \R^{C \times F}$ 表示有 $C\times F$ 个卷积核参数

>   上述是从频(谱)域角度对GCN的一个理解，特别地，GCN还可以从空域的角度进行解释，具体来说:
>
>   GCN的每一层可以看作对邻居节点的表示先进行一个特征变换($XW$) ，再进行一个聚合($AX$)，最终得到当前层的输出

## 空间域图卷积神经网络

频域方法中，采用傅里叶卷积定理定义卷积。而在空域中，则是从邻居节点信息聚合的角度出发，更加注重节点的局域环境。

### 图卷积神经网络的空域理解

如果我们从空域的角度出发，即不考虑严格的卷积定义，而是从邻居节点信息聚合的角度出发。图卷积神经网络，应该做的如下两件事情:

-   对节点的信息进行转换（Message Transformation）
-   对节点信息进行聚合 （Message Aggregation）

根据上面这两个点，那么 `GCN` 要做的事情可以表示为如下公式:
$$
h_v^{(l+1)} = \sigma(W^{l}\sum_{u \in N(u)}\frac{h_u^{(l)}}{\vert N(v) \vert} + h_v^{(l)}B^{(l)})
$$
其中，第一项 $W^{l}\sum_{u \in N(u)}\frac{h_u^{(l)}}{\vert N(v) \vert}$ 表示邻居节点信息的转换和聚合，第二项 $h_v^{(l)}B^{(l)}$ 表示自身节点信息的变换。

现在我们将上面的公式进行矩阵化。我们知道度矩阵 $D_{v,v} = Deg(v) = \vert N(v) \vert$, 因此 $D_{v, v}^{-1} = 1/\vert N(v) \vert$, 所以我们可以将 $\sum_{u\in N(v)}\frac{h_u^{(l)}}{\vert N(v) \vert}$ 转化为 $D^{-1}AH^{l}$, 更进一步，上述公式的矩阵表达可以写为:
$$
H^{(l+1)} = \sigma \Big(D^{-\frac{1}{2}}{A}D^{-\frac{1}{2}}H^{(l)}W^{(l)} + H^{(l)}B^{(l)}\Big)
$$
这个公式已经和`GCN`基本一致了，这里，我们分开表达他们是为了强调，一定要对邻居和自身节点都做信息变换和聚合。

### 空域图卷积的统一范式和 GraphSAGE

**图卷积的统一范式**

从空域图卷积神经网络必须做的两件事情出发，我们可以得到一个统一范式的图卷积网络。

-   对节点的信息进行转换（Message Transformation）
-   对节点信息进行聚合 （Message Aggregation）

下面的公式就是空域图卷积的统一范式：

$$
h_v^{(l+1)} = AGG_2^{l}\Big(AGG_1^{l}\big(\{TRANS_u^{(l)}(h_u^{(l)}), u\in N(v)\}\big), TRANS_v^{(l)}(h_u^{(l)})\Big)
$$

其中 $TRAN \ S_u$ 表示对邻居节点信息的转换，$TRAN \ S_v$ 表示对自身节点信息的转换，$AGG_1^{l}$ 表示对邻居节点信息的聚合，$AGG_2^{l}$ 表示对自身节点信息的聚合。

在这个统一范式下，在介绍 `GraphSAGE` 之前，我们简单介绍一下我们将会在后面章节学习的图注意力网络（Graph Attention Network, GAT）。我们可以看出来 `GCN` 在进行聚合的时候是没有考虑边的权重的而当作 `1` 进行简单的加和。`GAT` 的目的就是通过网络来学习边的权重，然后把学到的权重用于聚合。具体地，我们将边两端的节点送入一个网络，学习输出得到这条边的权重。

**GraphSAGE**

GraphSAGE 是 **SA**mple aggre**G**at**E** for **Graph**。其实上面的空域图卷积的统一视角也是 GraphSAGE 的作者提出来的。此外，它从两个方面对传统的 GCN 做了改进：

1.  在训练时，采样方式将 GCN 的全图采样优化到部分以节点为中心的邻居抽样，这使得大规模图数据的分布式训练成为可能，并且使得网络可以学习没有见过的节点，这也使得 GraphSAGE 可以做 Inductive Learning。
2.  GraphSAGE 研究了若干种邻居聚合的方式，及其 $AGG$ 聚合函数可以使用
    -   平均
    -   Max Pooling
    -   LSTM

在 GraphSAGE 之前的 GCN 模型中，都是采用的全图的训练方式，也就是说每一轮的迭代都要对全图的节点进行更新，当图的规模很大时，这种训练方式无疑是很耗时甚至无法更新的。mini-batch 的训练时深度学习一个非常重要的特点，那么能否将 mini-batch 的思想用到 GraphSAGE 中呢，GraphSAGE 提出了一个解决方案。它的流程大致分为3步：

1.  对邻居进行随机采样，每一跳抽样的邻居数不多于 $S_k$ 个；
2.  生成目标节点的 embedding：先聚合二跳邻居的特征，生成一跳邻居的embedding，再聚合一跳的 embedding，生成目标节点的 embedding；
3.  将目标节点的 embedding 输入全连接网络得到目标节点的预测值。

<img src="https://datawhalechina.github.io/grape-book/figures/05%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/5_graphsage_visual.png" alt="img" style="zoom: 50%;" />

下面是原论文的算法流程：

<img src="https://datawhalechina.github.io/grape-book/figures/05%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/5_graphsage_algorithm.png" alt="img" style="zoom: 50%;" />

>   **说明**
>
>   具体来说，GraphSAGE的每一层主要做了以下两件事:
>
>   1.   对邻居节点的信息进行聚合
>   2.   根据自身信息以及邻居节点的信息更新自身表示

## References

[1] Cheng-han Jiang, Hung-yi Lee (李宏毅), [机器学习：补充资料 Graph Neural Network](https://www.youtube.com/watch?v=eybCCtNKwzA&pp=ygUeZ3JhcGggbmV1cmFsIG5ldHdvcmsg5p2O5a6P5q-F)

[2] Jure Leskovec, [Stanford University CS224W: Machine Learning with Graphs](https://web.stanford.edu/class/cs224w/)

[3] [PyG 官方教程](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)

[4] [DGL 官方教程 - Link Prediction using Graph Neural Networks (GraphSAGE)](https://docs.dgl.ai/en/0.7.x/tutorials/blitz/4_link_predict.html)

[5] Kipf, T. N., & Welling, M. (2017). *Semi-Supervised Classification with Graph Convolutional Networks* (arXiv:1609.02907). arXiv. http://arxiv.org/abs/1609.02907

[6] DataWhale，[图深度学习 （葡萄书）第五章 图卷积神经网络](https://datawhalechina.github.io/grape-book/#/)
